"""
LLMService ì„œë¹„ìŠ¤

OpenAI, Anthropic, ë¡œì»¬ LLM ë“± ë‹¤ì–‘í•œ LLMì„ í†µí•© ê´€ë¦¬í•˜ëŠ” ë©”ì¸ ì„œë¹„ìŠ¤ì…ë‹ˆë‹¤.
"""

import logging
import asyncio
import time
from typing import Dict, List, Optional, Any
import os

from .models import (
    LLMInput, LLMResult, LLMProvider, ModelConfig,
    LLMResponseMetadata, ProviderConfig
)
from .exceptions import (
    LLMServiceException, UnsupportedProviderException,
    APICallFailedException, ProviderNotConfiguredException
)

logger = logging.getLogger(__name__)


class LLMService:
    """LLM ì„œë¹„ìŠ¤"""
    
    def __init__(self):
        self.providers = self._initialize_providers()
        self.default_provider = LLMProvider.OPENAI
        logger.info("LLMService initialized")
    
    def _initialize_providers(self) -> Dict[LLMProvider, ProviderConfig]:
        """ì œê³µìë³„ ì„¤ì • ì´ˆê¸°í™”"""
        providers = {}
        
        # OpenAI ì„¤ì •
        openai_key = os.getenv("OPENAI_API_KEY")
        if openai_key:
            providers[LLMProvider.OPENAI] = ProviderConfig(
                provider=LLMProvider.OPENAI,
                api_key=openai_key
            )
        
        # Anthropic ì„¤ì •
        anthropic_key = os.getenv("ANTHROPIC_API_KEY")
        if anthropic_key:
            providers[LLMProvider.ANTHROPIC] = ProviderConfig(
                provider=LLMProvider.ANTHROPIC,
                api_key=anthropic_key
            )
        
        return providers
    
    async def generate_summary(self, input_data: LLMInput) -> LLMResult:
        """ì£¼ìš” ìš”ì•½ ìƒì„± ë©”ì„œë“œ"""
        try:
            start_time = time.time()
            
            # Provider Selection
            if input_data.llm_provider not in self.providers:
                if input_data.llm_provider == LLMProvider.LOCAL:
                    return await self._generate_local_summary(input_data)
                else:
                    raise UnsupportedProviderException(input_data.llm_provider.value)
            
            # API Call
            response_text = await self._call_llm_api(input_data)
            
            # Response Processing
            response_time = time.time() - start_time
            metadata = self._create_metadata(input_data, response_time)
            
            # Quality Validation
            validated_response = self._validate_response(response_text)
            
            return LLMResult(
                summary=validated_response,
                metadata=metadata,
                confidence_score=0.85  # ê¸°ë³¸ ì‹ ë¢°ë„
            )
            
        except Exception as e:
            logger.error(f"Summary generation failed: {e}")
            raise APICallFailedException(input_data.llm_provider.value, e)
    
    async def _call_llm_api(self, input_data: LLMInput) -> str:
        """LLM API í˜¸ì¶œ"""
        provider_config = self.providers[input_data.llm_provider]
        
        if input_data.llm_provider == LLMProvider.OPENAI:
            return await self._call_openai_api(input_data, provider_config)
        elif input_data.llm_provider == LLMProvider.ANTHROPIC:
            return await self._call_anthropic_api(input_data, provider_config)
        else:
            raise UnsupportedProviderException(input_data.llm_provider.value)
    
    async def _call_openai_api(self, input_data: LLMInput, 
                              config: ProviderConfig) -> str:
        """OpenAI API í˜¸ì¶œ"""
        try:
            # ì‹¤ì œ OpenAI API í˜¸ì¶œ ëŒ€ì‹  Mock ì‘ë‹µ
            await asyncio.sleep(0.5)  # API í˜¸ì¶œ ì‹œë®¬ë ˆì´ì…˜
            
            return f"""
{input_data.prompt}ì— ëŒ€í•œ AI ë¶„ì„ ê²°ê³¼:

ğŸ“Š ì¢…í•© ë¶„ì„:
- ê°œë°œ í™œë™ì´ í™œë°œí•˜ê²Œ ì§„í–‰ë˜ì—ˆìŠµë‹ˆë‹¤.
- ì½”ë“œ í’ˆì§ˆì´ ì–‘í˜¸í•œ ìˆ˜ì¤€ì„ ìœ ì§€í•˜ê³  ìˆìŠµë‹ˆë‹¤.
- ì‘ì—… íŒ¨í„´ì´ ì¼ê´€ì„± ìˆê²Œ ê´€ë¦¬ë˜ê³  ìˆìŠµë‹ˆë‹¤.

ğŸ’¡ ì£¼ìš” ì¸ì‚¬ì´íŠ¸:
- íš¨ìœ¨ì ì¸ ê°œë°œ í”„ë¡œì„¸ìŠ¤ê°€ í™•ì¸ë©ë‹ˆë‹¤.
- ì§€ì†ì ì¸ ê°œì„ ì´ ì´ë£¨ì–´ì§€ê³  ìˆìŠµë‹ˆë‹¤.

ğŸ¯ ê¶Œì¥ì‚¬í•­:
- í˜„ì¬ ìˆ˜ì¤€ì„ ìœ ì§€í•˜ë©° ì ì§„ì  ê°œì„ ì„ ì¶”ì²œí•©ë‹ˆë‹¤.
            """.strip()
            
        except Exception as e:
            raise APICallFailedException("openai", e)
    
    async def _call_anthropic_api(self, input_data: LLMInput, 
                                 config: ProviderConfig) -> str:
        """Anthropic API í˜¸ì¶œ"""
        try:
            # ì‹¤ì œ Anthropic API í˜¸ì¶œ ëŒ€ì‹  Mock ì‘ë‹µ
            await asyncio.sleep(0.7)  # API í˜¸ì¶œ ì‹œë®¬ë ˆì´ì…˜
            
            return f"""
Claude ë¶„ì„ ê²°ê³¼:

{input_data.prompt}ì— ëŒ€í•œ ìƒì„¸ ë¶„ì„ì„ ì œê³µí•©ë‹ˆë‹¤.

ğŸ” ë¶„ì„ ìš”ì•½:
ê°œë°œìì˜ í™œë™ íŒ¨í„´ê³¼ ì½”ë“œ í’ˆì§ˆì„ ì¢…í•©ì ìœ¼ë¡œ ë¶„ì„í•œ ê²°ê³¼, 
ì „ë°˜ì ìœ¼ë¡œ ìš°ìˆ˜í•œ ê°œë°œ ì„±ê³¼ë¥¼ ë³´ì´ê³  ìˆìŠµë‹ˆë‹¤.

ğŸ“ˆ ì„±ê³¼ ì§€í‘œ:
- ìƒì‚°ì„±: ë†’ìŒ
- ì½”ë“œ í’ˆì§ˆ: ì–‘í˜¸
- ì¼ê´€ì„±: ìš°ìˆ˜

ğŸ’­ ê°œì„  ì œì•ˆ:
ì§€ì†ì ì¸ ë°œì „ì„ ìœ„í•œ êµ¬ì²´ì ì¸ ë°©í–¥ì„±ì„ ì œì‹œí•©ë‹ˆë‹¤.
            """.strip()
            
        except Exception as e:
            raise APICallFailedException("anthropic", e)
    
    async def _generate_local_summary(self, input_data: LLMInput) -> LLMResult:
        """ë¡œì»¬ LLMì„ ì‚¬ìš©í•œ ìš”ì•½ ìƒì„±"""
        try:
            start_time = time.time()
            
            # ê°„ë‹¨í•œ ë¡œì»¬ ì²˜ë¦¬
            summary = f"""
ë¡œì»¬ LLM ë¶„ì„ ê²°ê³¼:

ì…ë ¥ëœ í”„ë¡¬í”„íŠ¸ë¥¼ ë¶„ì„í•˜ì—¬ ë‹¤ìŒê³¼ ê°™ì€ ìš”ì•½ì„ ì œê³µí•©ë‹ˆë‹¤:

ğŸ“‹ ìš”ì•½:
- ê°œë°œ í™œë™ì´ ì •ìƒì ìœ¼ë¡œ ìˆ˜í–‰ë˜ì—ˆìŠµë‹ˆë‹¤.
- ê¸°ë³¸ì ì¸ í’ˆì§ˆ ê¸°ì¤€ì„ ì¶©ì¡±í•˜ê³  ìˆìŠµë‹ˆë‹¤.

âš ï¸ ì°¸ê³ :
ì´ ê²°ê³¼ëŠ” ë¡œì»¬ ì²˜ë¦¬ëœ ê°„ë‹¨í•œ ë¶„ì„ì…ë‹ˆë‹¤.
            """.strip()
            
            response_time = time.time() - start_time
            
            metadata = LLMResponseMetadata(
                token_usage={"prompt_tokens": 100, "completion_tokens": 150, "total_tokens": 250},
                response_time=response_time,
                model_used="local-model",
                provider="local"
            )
            
            return LLMResult(
                summary=summary,
                metadata=metadata,
                confidence_score=0.6  # ë¡œì»¬ ëª¨ë¸ì€ ë‚®ì€ ì‹ ë¢°ë„
            )
            
        except Exception as e:
            raise APICallFailedException("local", e)
    
    def switch_provider(self, new_provider: LLMProvider):
        """LLM ì œê³µì ë™ì  ë³€ê²½"""
        if new_provider not in self.providers and new_provider != LLMProvider.LOCAL:
            raise ProviderNotConfiguredException(new_provider.value)
        
        self.default_provider = new_provider
        logger.info(f"Switched to provider: {new_provider.value}")
    
    def validate_response(self, response: str) -> bool:
        """ì‘ë‹µ í’ˆì§ˆ ê²€ì¦"""
        return self._validate_response(response) is not None
    
    def handle_rate_limits(self, provider: LLMProvider) -> int:
        """API ì œí•œ ì²˜ë¦¬"""
        # ê°„ë‹¨í•œ ì¬ì‹œë„ ì§€ì—° ë°˜í™˜
        return 60  # 60ì´ˆ ëŒ€ê¸°
    
    def _create_metadata(self, input_data: LLMInput, response_time: float) -> LLMResponseMetadata:
        """ë©”íƒ€ë°ì´í„° ìƒì„±"""
        estimated_prompt_tokens = len(input_data.prompt.split()) * 1.3
        estimated_completion_tokens = 200  # ê¸°ë³¸ê°’
        
        return LLMResponseMetadata(
            token_usage={
                "prompt_tokens": int(estimated_prompt_tokens),
                "completion_tokens": estimated_completion_tokens,
                "total_tokens": int(estimated_prompt_tokens) + estimated_completion_tokens
            },
            response_time=response_time,
            model_used=input_data.model_config.model,
            provider=input_data.llm_provider.value,
            cost_estimate=0.02  # ê¸°ë³¸ ë¹„ìš© ì¶”ì •
        )
    
    def _validate_response(self, response: str) -> Optional[str]:
        """ì‘ë‹µ ê²€ì¦"""
        if not response or len(response.strip()) < 10:
            return None
        
        # ê¸°ë³¸ì ì¸ í•„í„°ë§
        if "error" in response.lower() or "failed" in response.lower():
            return None
        
        return response.strip()
    
    def get_available_providers(self) -> List[LLMProvider]:
        """ì‚¬ìš© ê°€ëŠ¥í•œ ì œê³µì ëª©ë¡"""
        available = list(self.providers.keys())
        available.append(LLMProvider.LOCAL)  # ë¡œì»¬ì€ í•­ìƒ ì‚¬ìš© ê°€ëŠ¥
        return available
    
    def is_provider_configured(self, provider: LLMProvider) -> bool:
        """ì œê³µì ì„¤ì • ì—¬ë¶€ í™•ì¸"""
        return provider in self.providers or provider == LLMProvider.LOCAL 