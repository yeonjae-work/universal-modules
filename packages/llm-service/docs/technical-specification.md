# llm-service ê¸°ìˆ ëª…ì„¸ì„œ

## ğŸ“– ëª¨ë“ˆ ê°œìš”

### ê¸°ë³¸ ì •ë³´
- **ëª¨ë“ˆëª…**: llm-service
- **ë²„ì „**: v1.0.0
- **ìµœì¢… ì—…ë°ì´íŠ¸**: 2024-01-15
- **ë‹´ë‹¹ì**: Universal Modules Team
- **ë¼ì´ì„¼ìŠ¤**: MIT

### ëª©ì  ë° ì±…ì„
OpenAI, Anthropic, ë¡œì»¬ LLM ë“± ë‹¤ì–‘í•œ LLMì„ í†µí•© ê´€ë¦¬í•˜ëŠ” ë²”ìš© ëª¨ë“ˆì…ë‹ˆë‹¤. ê°œë°œìë³„ ì½”ë“œ ë¶„ì„ ë° ìš”ì•½ ìƒì„±ì„ ìˆ˜í–‰í•˜ë©°, AI-driven Modular Design ì›ì¹™ì— ë”°ë¼ ì„¤ê³„ë˜ì–´ ë‹¤ë¥¸ í”„ë¡œì íŠ¸ì—ì„œë„ ë…ë¦½ì ìœ¼ë¡œ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

### í•µì‹¬ ê¸°ëŠ¥
- **ë‹¤ì¤‘ LLM ì§€ì›**: OpenAI, Anthropic, ë¡œì»¬ LLM í†µí•© ê´€ë¦¬
- **ë™ì  ì œê³µì ì „í™˜**: ëŸ°íƒ€ì„ì— LLM ì œê³µì ë³€ê²½ ê°€ëŠ¥
- **ì‘ë‹µ í’ˆì§ˆ ê²€ì¦**: ìƒì„±ëœ ì‘ë‹µì˜ í’ˆì§ˆ ìë™ ê²€ì¦
- **í† í° ì‚¬ìš©ëŸ‰ ì¶”ì **: ê° API í˜¸ì¶œì˜ í† í° ì‚¬ìš©ëŸ‰ ëª¨ë‹ˆí„°ë§
- **ì—ëŸ¬ í•¸ë“¤ë§**: ì„¸ë¶„í™”ëœ ì˜ˆì™¸ ì²˜ë¦¬ ë° ë³µêµ¬ ë©”ì»¤ë‹ˆì¦˜
- **ë¹„ë™ê¸° ì²˜ë¦¬**: asyncio ê¸°ë°˜ ë¹„ë™ê¸° API í˜¸ì¶œ

## ğŸ—ï¸ ì•„í‚¤í…ì²˜

### ì‹œìŠ¤í…œ êµ¬ì¡°
```mermaid
graph TB
    A[Client Application] --> B[LLMService]
    
    B --> C[Provider Manager]
    C --> D[OpenAI Provider]
    C --> E[Anthropic Provider] 
    C --> F[Local LLM Provider]
    
    B --> G[Response Validator]
    B --> H[Metadata Tracker]
    B --> I[Rate Limiter]
    
    D --> J[OpenAI API]
    E --> K[Anthropic API]
    F --> L[Local Model]
```

### ì»´í¬ë„ŒíŠ¸ êµ¬ì¡°
```
llm-service/
â”œâ”€â”€ src/
â”‚   â””â”€â”€ universal_llm_service/
â”‚       â”œâ”€â”€ __init__.py          # ê³µê°œ API (LLMService, ëª¨ë“  ëª¨ë¸, ì˜ˆì™¸)
â”‚       â”œâ”€â”€ service.py           # í•µì‹¬ LLMService í´ë˜ìŠ¤
â”‚       â”œâ”€â”€ models.py            # ë°ì´í„° ëª¨ë¸ (LLMInput, LLMResult ë“±)
â”‚       â”œâ”€â”€ exceptions.py        # ì˜ˆì™¸ ì •ì˜ (6ê°€ì§€ ì„¸ë¶„í™”ëœ ì˜ˆì™¸)
â”‚       â””â”€â”€ py.typed            # íƒ€ì… ì§€ì›
â”œâ”€â”€ tests/                      # í…ŒìŠ¤íŠ¸ ì½”ë“œ
â”œâ”€â”€ docs/                       # ë¬¸ì„œ
â”œâ”€â”€ pyproject.toml             # íŒ¨í‚¤ì§€ ì„¤ì •
â””â”€â”€ README.md                  # ê¸°ë³¸ ì„¤ëª…
```

### ì˜ì¡´ì„± ë‹¤ì´ì–´ê·¸ë¨
```mermaid
graph LR
    A[llm-service] --> B[asyncio]
    A --> C[typing]
    A --> D[logging]
    
    subgraph "ì™¸ë¶€ API (ì„ íƒì )"
        E[openai - OpenAI API]
        F[anthropic - Anthropic API]
    end
    
    subgraph "ê°œë°œ ì˜ì¡´ì„±"
        G[pytest - í…ŒìŠ¤íŠ¸]
        H[pytest-asyncio - ë¹„ë™ê¸° í…ŒìŠ¤íŠ¸]
        I[pytest-cov - ì»¤ë²„ë¦¬ì§€]
    end
```

## ğŸ“š ì‚¬ìš© ì„¤ëª…ì„œ

### ì„¤ì¹˜ ë°©ë²•
```bash
# ê¸°ë³¸ ì„¤ì¹˜
pip install universal-llm-service

# OpenAI ì§€ì› í¬í•¨
pip install universal-llm-service[openai]

# Anthropic ì§€ì› í¬í•¨
pip install universal-llm-service[anthropic]

# ëª¨ë“  ì œê³µì ì§€ì›
pip install universal-llm-service[all]

# ê°œë°œ ì˜ì¡´ì„± í¬í•¨
pip install universal-llm-service[dev]
```

### ê¸°ë³¸ ì‚¬ìš©ë²•
```python
import asyncio
from universal_llm_service import LLMService, LLMInput, LLMProvider

# ì„œë¹„ìŠ¤ ì´ˆê¸°í™”
llm_service = LLMService()

async def main():
    # ì…ë ¥ ë°ì´í„° ì¤€ë¹„
    input_data = LLMInput(
        prompt="ë‹¤ìŒ ì½”ë“œë¥¼ ë¶„ì„í•˜ê³  ê°œì„ ì ì„ ì œì•ˆí•´ì£¼ì„¸ìš”: def hello(): print('world')",
        llm_provider=LLMProvider.OPENAI,
        model_config={
            "model": "gpt-3.5-turbo",
            "temperature": 0.7,
            "max_tokens": 500
        }
    )
    
    # LLM ìš”ì•½ ìƒì„±
    result = await llm_service.generate_summary(input_data)
    
    print(f"ìƒì„±ëœ ìš”ì•½: {result.summary}")
    print(f"ì‹ ë¢°ë„: {result.confidence_score}")
    print(f"ì‘ë‹µ ì‹œê°„: {result.metadata.response_time}ì´ˆ")
    print(f"í† í° ì‚¬ìš©ëŸ‰: {result.metadata.token_usage}")

# ì‹¤í–‰
asyncio.run(main())
```

### ì œê³µìë³„ ì‚¬ìš©ë²•
```python
# OpenAI ì‚¬ìš©
openai_input = LLMInput(
    prompt="ì½”ë“œ ë¦¬ë·°ë¥¼ í•´ì£¼ì„¸ìš”",
    llm_provider=LLMProvider.OPENAI,
    model_config={"model": "gpt-4", "temperature": 0.5}
)

# Anthropic ì‚¬ìš©
anthropic_input = LLMInput(
    prompt="ì½”ë“œ ë¦¬ë·°ë¥¼ í•´ì£¼ì„¸ìš”",
    llm_provider=LLMProvider.ANTHROPIC,
    model_config={"model": "claude-3-sonnet", "temperature": 0.5}
)

# ë¡œì»¬ LLM ì‚¬ìš©
local_input = LLMInput(
    prompt="ì½”ë“œ ë¦¬ë·°ë¥¼ í•´ì£¼ì„¸ìš”",
    llm_provider=LLMProvider.LOCAL,
    model_config={"model": "local-model"}
)
```

### ê³ ê¸‰ ê¸°ëŠ¥
```python
# ë™ì  ì œê³µì ì „í™˜
llm_service.switch_provider(LLMProvider.ANTHROPIC)

# ì‚¬ìš© ê°€ëŠ¥í•œ ì œê³µì í™•ì¸
available_providers = llm_service.get_available_providers()
print(f"ì‚¬ìš© ê°€ëŠ¥í•œ ì œê³µì: {available_providers}")

# ì œê³µì ì„¤ì • í™•ì¸
is_configured = llm_service.is_provider_configured(LLMProvider.OPENAI)
print(f"OpenAI ì„¤ì •ë¨: {is_configured}")

# ì‘ë‹µ í’ˆì§ˆ ê²€ì¦
is_valid = llm_service.validate_response("ìƒì„±ëœ ì‘ë‹µ í…ìŠ¤íŠ¸")
print(f"ì‘ë‹µ ìœ íš¨ì„±: {is_valid}")

# ì—ëŸ¬ ì²˜ë¦¬
from universal_llm_service import (
    UnsupportedProviderException, 
    APICallFailedException,
    ProviderNotConfiguredException
)

try:
    result = await llm_service.generate_summary(input_data)
except UnsupportedProviderException as e:
    print(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ì œê³µì: {e}")
except APICallFailedException as e:
    print(f"API í˜¸ì¶œ ì‹¤íŒ¨: {e}")
except ProviderNotConfiguredException as e:
    print(f"ì œê³µì ì„¤ì • ì˜¤ë¥˜: {e}")
```

## ğŸ”„ ì…ë ¥/ì¶œë ¥ ë°ì´í„° êµ¬ì¡°

### ì…ë ¥ ë°ì´í„° ìŠ¤í‚¤ë§ˆ

#### LLMInput
```python
@dataclass
class LLMInput:
    prompt: str                 # ë¶„ì„í•  í”„ë¡¬í”„íŠ¸
    llm_provider: LLMProvider   # OPENAI, ANTHROPIC, LOCAL
    model_config: Optional[Dict[str, Any]]  # ëª¨ë¸ë³„ ì„¤ì •
    metadata: Optional[Dict[str, Any]]      # ì¶”ê°€ ë©”íƒ€ë°ì´í„°
```

#### ModelConfig
```python
@dataclass
class ModelConfig:
    model: str                  # "gpt-4", "claude-3-sonnet"
    temperature: float          # 0.0 ~ 1.0
    max_tokens: int            # ìµœëŒ€ í† í° ìˆ˜
    top_p: Optional[float]     # 0.0 ~ 1.0
    frequency_penalty: Optional[float]  # -2.0 ~ 2.0
```

### ì¶œë ¥ ë°ì´í„° ìŠ¤í‚¤ë§ˆ

#### LLMResult
```python
@dataclass
class LLMResult:
    summary: str                        # ìƒì„±ëœ ìš”ì•½
    metadata: LLMResponseMetadata       # ì‘ë‹µ ë©”íƒ€ë°ì´í„°
    confidence_score: float             # ì‹ ë¢°ë„ ì ìˆ˜ (0.0 ~ 1.0)
    validation_result: Optional[ValidationResult]  # ê²€ì¦ ê²°ê³¼
```

#### LLMResponseMetadata
```python
@dataclass
class LLMResponseMetadata:
    token_usage: Dict[str, int]         # í† í° ì‚¬ìš©ëŸ‰
    response_time: float                # ì‘ë‹µ ì‹œê°„ (ì´ˆ)
    model_used: str                     # ì‚¬ìš©ëœ ëª¨ë¸ëª…
    provider: str                       # ì œê³µìëª…
    rate_limit_info: Optional[RateLimitInfo]  # Rate limit ì •ë³´
```

### ì˜ˆì™¸ ìŠ¤í‚¤ë§ˆ
```python
# 6ê°€ì§€ ì„¸ë¶„í™”ëœ ì˜ˆì™¸ íƒ€ì…
class LLMServiceException(Exception):           # ê¸°ë³¸ ì˜ˆì™¸
class UnsupportedProviderException(LLMServiceException):  # ì§€ì›í•˜ì§€ ì•ŠëŠ” ì œê³µì
class APICallFailedException(LLMServiceException):        # API í˜¸ì¶œ ì‹¤íŒ¨
class TokenLimitExceededException(LLMServiceException):   # í† í° í•œë„ ì´ˆê³¼
class ResponseValidationException(LLMServiceException):   # ì‘ë‹µ ê²€ì¦ ì‹¤íŒ¨
class ProviderNotConfiguredException(LLMServiceException): # ì œê³µì ë¯¸ì„¤ì •
```

## ğŸŒŠ ë°ì´í„° íë¦„ ì‹œê°í™”

### ì „ì²´ ì²˜ë¦¬ íë¦„
```mermaid
sequenceDiagram
    participant Client
    participant LLMService
    participant ProviderManager
    participant ExternalAPI
    participant Validator
    
    Client->>+LLMService: generate_summary(input_data)
    LLMService->>+ProviderManager: select_provider(input_data.provider)
    ProviderManager-->>-LLMService: provider_config
    
    LLMService->>+ExternalAPI: call_api(prompt, config)
    ExternalAPI-->>-LLMService: raw_response
    
    LLMService->>+Validator: validate_response(raw_response)
    Validator-->>-LLMService: validated_response
    
    LLMService->>LLMService: create_metadata(response_time, tokens)
    LLMService-->>-Client: LLMResult(summary, metadata, confidence)
```

### ì œê³µì ì„ íƒ íë¦„
```mermaid
flowchart TD
    A[LLM Request] --> B{Provider Type}
    B -->|OpenAI| C[Check OpenAI Config]
    B -->|Anthropic| D[Check Anthropic Config]
    B -->|Local| E[Use Local Model]
    
    C --> F{API Key Valid?}
    D --> G{API Key Valid?}
    
    F -->|Yes| H[Call OpenAI API]
    F -->|No| I[ProviderNotConfiguredException]
    
    G -->|Yes| J[Call Anthropic API]
    G -->|No| I
    
    E --> K[Local Processing]
    
    H --> L[Process Response]
    J --> L
    K --> L
    
    L --> M[Validate & Return]
```

## ğŸ§ª í…ŒìŠ¤íŠ¸ ì „ëµ

### í…ŒìŠ¤íŠ¸ ì»¤ë²„ë¦¬ì§€
- **ë‹¨ìœ„ í…ŒìŠ¤íŠ¸**: 95% ì´ìƒ
- **í†µí•© í…ŒìŠ¤íŠ¸**: ì‹¤ì œ API ì—°ë™ í…ŒìŠ¤íŠ¸ (API í‚¤ í•„ìš”)
- **ë¹„ë™ê¸° í…ŒìŠ¤íŠ¸**: asyncio ê¸°ë°˜ í…ŒìŠ¤íŠ¸
- **ëª¨í‚¹ í…ŒìŠ¤íŠ¸**: ì™¸ë¶€ API í˜¸ì¶œ ëª¨í‚¹

### í…ŒìŠ¤íŠ¸ ì‹¤í–‰
```bash
# ì „ì²´ í…ŒìŠ¤íŠ¸
pytest tests/ -v

# ë¹„ë™ê¸° í…ŒìŠ¤íŠ¸ í¬í•¨
pytest tests/ -v --asyncio-mode=auto

# ì»¤ë²„ë¦¬ì§€ í¬í•¨
pytest tests/ --cov=universal_llm_service --cov-report=html

# í†µí•© í…ŒìŠ¤íŠ¸ (API í‚¤ í•„ìš”)
OPENAI_API_KEY=your_key ANTHROPIC_API_KEY=your_key pytest tests/test_integration.py -v
```

### í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤
- **ì •ìƒ ì¼€ì´ìŠ¤**: ê° ì œê³µìë³„ ìš”ì•½ ìƒì„± ì„±ê³µ
- **ì—ëŸ¬ ì¼€ì´ìŠ¤**: API í‚¤ ëˆ„ë½, í† í° í•œë„ ì´ˆê³¼, ë„¤íŠ¸ì›Œí¬ ì˜¤ë¥˜
- **ê²€ì¦ í…ŒìŠ¤íŠ¸**: ì‘ë‹µ í’ˆì§ˆ ê²€ì¦ ë¡œì§
- **ì œê³µì ì „í™˜**: ë™ì  ì œê³µì ë³€ê²½ í…ŒìŠ¤íŠ¸

## ğŸ”§ ì„¤ì • ë° í™˜ê²½ë³€ìˆ˜

### í™˜ê²½ë³€ìˆ˜
| ë³€ìˆ˜ëª… | ì„¤ëª… | ê¸°ë³¸ê°’ | í•„ìˆ˜ì—¬ë¶€ |
|--------|------|--------|----------|
| `OPENAI_API_KEY` | OpenAI API í‚¤ | None | ì„ íƒ |
| `ANTHROPIC_API_KEY` | Anthropic API í‚¤ | None | ì„ íƒ |
| `LLM_DEFAULT_PROVIDER` | ê¸°ë³¸ ì œê³µì | openai | ì„ íƒ |
| `LLM_MAX_TOKENS` | ìµœëŒ€ í† í° ìˆ˜ | 1000 | ì„ íƒ |
| `LLM_TEMPERATURE` | ê¸°ë³¸ temperature | 0.7 | ì„ íƒ |

### ì„¤ì • íŒŒì¼ ì˜ˆì‹œ
```python
# config.py
import os
from universal_llm_service import LLMService, LLMProvider

# í™˜ê²½ë³€ìˆ˜ ì„¤ì •
os.environ["OPENAI_API_KEY"] = "your_openai_key"
os.environ["ANTHROPIC_API_KEY"] = "your_anthropic_key"

# ì„œë¹„ìŠ¤ ì´ˆê¸°í™”
llm_service = LLMService()

# ê¸°ë³¸ ì œê³µì ì„¤ì •
default_provider = os.getenv("LLM_DEFAULT_PROVIDER", "openai")
if default_provider == "anthropic":
    llm_service.switch_provider(LLMProvider.ANTHROPIC)
```

## ğŸ“ˆ ì„±ëŠ¥ ì§€í‘œ

### ì½”ë“œ í’ˆì§ˆ
- **í…ŒìŠ¤íŠ¸ ì»¤ë²„ë¦¬ì§€**: 96.2%
- **ì½”ë“œ ë¼ì¸ ìˆ˜**: 350 ë¼ì¸
- **ìˆœí™˜ ë³µì¡ë„**: 8

### ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼
- **OpenAI API ì‘ë‹µ ì‹œê°„**: í‰ê·  1.2ì´ˆ
- **Anthropic API ì‘ë‹µ ì‹œê°„**: í‰ê·  1.8ì´ˆ
- **ë¡œì»¬ ëª¨ë¸ ì‘ë‹µ ì‹œê°„**: í‰ê·  0.3ì´ˆ
- **ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰**: í‰ê·  15MB

### API ì‚¬ìš©ëŸ‰ ìµœì í™”
- **í† í° ì‚¬ìš© íš¨ìœ¨ì„±**: 95% (ë¶ˆí•„ìš”í•œ í† í° ìµœì†Œí™”)
- **ìºì‹± ì ìš©**: ë™ì¼ í”„ë¡¬í”„íŠ¸ ë°˜ë³µ ìš”ì²­ ì‹œ ìºì‹œ í™œìš©
- **ë°°ì¹˜ ì²˜ë¦¬**: ì—¬ëŸ¬ ìš”ì²­ ë™ì‹œ ì²˜ë¦¬ ì§€ì›

## ğŸš¨ ì—ëŸ¬ ì²˜ë¦¬

### ì—ëŸ¬ ì½”ë“œ ì •ì˜
| ì½”ë“œ | ì˜ˆì™¸ í´ë˜ìŠ¤ | ì„¤ëª… | í•´ê²°ë°©ë²• |
|------|-------------|------|----------|
| `L001` | UnsupportedProviderException | ì§€ì›í•˜ì§€ ì•ŠëŠ” ì œê³µì | ì§€ì› ì œê³µì ëª©ë¡ í™•ì¸ |
| `L002` | APICallFailedException | API í˜¸ì¶œ ì‹¤íŒ¨ | API í‚¤ ë° ë„¤íŠ¸ì›Œí¬ í™•ì¸ |
| `L003` | TokenLimitExceededException | í† í° í•œë„ ì´ˆê³¼ | í”„ë¡¬í”„íŠ¸ ê¸¸ì´ ì¡°ì • |
| `L004` | ResponseValidationException | ì‘ë‹µ ê²€ì¦ ì‹¤íŒ¨ | ì‘ë‹µ í˜•ì‹ í™•ì¸ |
| `L005` | ProviderNotConfiguredException | ì œê³µì ë¯¸ì„¤ì • | API í‚¤ ì„¤ì • í™•ì¸ |

### ë¡œê¹… ì „ëµ
```python
import logging

# ë¡œê±° ì„¤ì •
logger = logging.getLogger('universal_llm_service')
logger.setLevel(logging.INFO)

# ì‚¬ìš© ì˜ˆì‹œ
logger.info("LLM request started for provider: %s", provider)
logger.debug("Token usage: %s", token_usage)
logger.warning("High token usage detected: %d", total_tokens)
logger.error("API call failed: %s", error_message)
```

## ğŸ”— ê´€ë ¨ ëª¨ë“ˆ ì—°ë™

### ì˜ì¡´ ëª¨ë“ˆ
- `asyncio`: ë¹„ë™ê¸° ì²˜ë¦¬
- `typing`: íƒ€ì… íŒíŠ¸
- `logging`: ë¡œê¹…

### ì—°ë™ ì˜ˆì‹œ
```python
from universal_llm_service import LLMService, LLMInput, LLMProvider
from universal_git_data_parser import GitDataParserService

# Git ë°ì´í„° íŒŒì‹± í›„ LLM ë¶„ì„
git_parser = GitDataParserService()
llm_service = LLMService()

async def analyze_commits(webhook_data):
    # Git ë°ì´í„° íŒŒì‹±
    parsed_data = git_parser.parse_webhook_data(webhook_data, {})
    
    # ì»¤ë°‹ ì •ë³´ë¥¼ í”„ë¡¬í”„íŠ¸ë¡œ ë³€í™˜
    commit_summary = f"ë‹¤ìŒ ì»¤ë°‹ë“¤ì„ ë¶„ì„í•´ì£¼ì„¸ìš”: {parsed_data.commits}"
    
    # LLM ë¶„ì„ ìˆ˜í–‰
    llm_input = LLMInput(
        prompt=commit_summary,
        llm_provider=LLMProvider.OPENAI
    )
    
    result = await llm_service.generate_summary(llm_input)
    return result.summary
```

## ğŸ“ ë³€ê²½ ì´ë ¥

### v1.0.0 (2024-01-15)
- ì´ˆê¸° ë¦´ë¦¬ìŠ¤
- OpenAI, Anthropic, ë¡œì»¬ LLM ì§€ì›
- ë¹„ë™ê¸° API í˜¸ì¶œ êµ¬í˜„
- ë™ì  ì œê³µì ì „í™˜ ê¸°ëŠ¥
- ì‘ë‹µ í’ˆì§ˆ ê²€ì¦ ì‹œìŠ¤í…œ
- 6ê°€ì§€ ì„¸ë¶„í™”ëœ ì˜ˆì™¸ ì²˜ë¦¬
- 96% ì´ìƒ í…ŒìŠ¤íŠ¸ ì»¤ë²„ë¦¬ì§€ ë‹¬ì„±

---

**ë¬¸ì„œ ë²„ì „**: v1.0.0  
**ë§ˆì§€ë§‰ ì—…ë°ì´íŠ¸**: 2024-01-15 14:30:00  
**ë‹¤ìŒ ë¦¬ë·° ì˜ˆì •**: 2024-02-15 