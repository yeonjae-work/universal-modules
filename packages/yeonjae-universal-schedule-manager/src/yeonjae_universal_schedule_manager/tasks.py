"""Celery tasks for schedule manager."""

from __future__ import annotations

import logging
from datetime import datetime, timedelta
from typing import Dict, Any

from celery import shared_task

from yeonjae_universal_data_retriever.service import DataRetrieverService
from yeonjae_universal_llm_service import LLMService, LLMInput, LLMProvider, ModelConfig
from yeonjae_universal_data_aggregator.service import DataAggregatorService
from yeonjae_universal_prompt_builder.service import PromptBuilderService
from yeonjae_universal_notification_service import NotificationService
# Simplified logging for standalone operation
def log_module_io(module_name: str, operation: str, data: dict):
    pass

logger = logging.getLogger(__name__)


@shared_task(name="schedule_manager.daily_summary_task")
def daily_summary_task() -> Dict[str, Any]:
    """
    ë§¤ì¼ ì˜¤ì „ 8ì‹œì— ì‹¤í–‰ë˜ëŠ” ì¼ì¼ ê°œë°œì í™œë™ ìš”ì•½ ì‘ì—…
    
    ê¸°íšì„œ ì¤€ìˆ˜ íë¦„:
    ScheduleManager â†’ DataRetriever â†’ DataAggregator â†’ PromptBuilder â†’ LLMService â†’ NotificationService
    
    Returns:
        Dict[str, Any]: ì‹¤í–‰ ê²°ê³¼
    """
    start_time = datetime.now()
    
    try:
        logger.info("ğŸŒ… Starting daily developer summary at 08:00 Asia/Seoul")
        
        # ì–´ì œ ë‚ ì§œ ë²”ìœ„ ê³„ì‚° (Asia/Seoul ê¸°ì¤€)
        from zoneinfo import ZoneInfo
        seoul_tz = ZoneInfo("Asia/Seoul")
        now_seoul = datetime.now(seoul_tz)
        yesterday_start = (now_seoul - timedelta(days=1)).replace(hour=0, minute=0, second=0, microsecond=0)
        yesterday_end = (now_seoul - timedelta(days=1)).replace(hour=23, minute=59, second=59, microsecond=999999)
        
        logger.info(f"ğŸ“… Processing data for: {yesterday_start.strftime('%Y-%m-%d')} (Asia/Seoul)")
        
        # 1ï¸âƒ£ DataRetriever: ì–´ì œ ë°ì´í„° ì¡°íšŒ
        logger.info("ğŸ“Š DataRetriever: Starting data retrieval")
        
        log_module_io("DataRetriever", "INPUT", metadata={
            "operation": "get_daily_summary",
            "target_date": yesterday_start.isoformat(),
            "timezone": "Asia/Seoul"
        })
        
        data_retriever = DataRetrieverService()
        retrieved_data = data_retriever.get_daily_summary(target_date=yesterday_start)
        
        log_module_io("DataRetriever", "OUTPUT", output_data=retrieved_data, metadata={
            "commits_found": len(retrieved_data.commits),
            "developers_count": len(set(commit.author for commit in retrieved_data.commits)),
            "query_time": retrieved_data.metadata.query_time_seconds if retrieved_data.metadata else 0
        })
        
        logger.info(
            "âœ… DataRetriever: Found %d records from daily summary",
            len(retrieved_data.commits)
        )
        
        # ë°ì´í„°ê°€ ì—†ìœ¼ë©´ ì¢…ë£Œ
        if not retrieved_data.commits:
            logger.info("ğŸ“­ No data found for yesterday. Skipping daily summary.")
            return {
                "success": True,
                "message": "No data found for yesterday",
                "execution_time_seconds": (datetime.now() - start_time).total_seconds()
            }
        
        # 2ï¸âƒ£ DataAggregator: ê°œë°œìë³„ í™œë™ ì§‘ê³„
        logger.info("ğŸ”¢ DataAggregator: Starting data aggregation")
        
        # ë°ì´í„°ë¥¼ AggregationInput í˜•íƒœë¡œ ë³€í™˜
        from yeonjae_universal_data_aggregator.models import AggregationInput, CommitData, DiffInfo, DateRange
        
        commits = []
        for commit_info in retrieved_data.commits:
            # CommitInfoë¥¼ CommitDataë¡œ ë³€í™˜
            commit_data = CommitData(
                commit_id=commit_info.commit_hash,
                author=commit_info.author,
                author_email=commit_info.author_email,
                timestamp=commit_info.timestamp,
                message=commit_info.message,
                repository=commit_info.repository,
                branch=commit_info.branch,
                diff_info=[]  # ê¸°ë³¸ê°’
            )
            commits.append(commit_data)
        
        # DateRangeë¥¼ ë¬¸ìì—´ í˜•ì‹ìœ¼ë¡œ ìƒì„±
        date_range = DateRange(
            start=yesterday_start.strftime('%Y-%m-%d'),
            end=yesterday_end.strftime('%Y-%m-%d')
        )
        
        aggregation_input = AggregationInput(
            commits=commits,
            date_range=date_range
        )
        
        log_module_io("DataAggregator", "INPUT", input_data=aggregation_input, metadata={
            "operation": "aggregate_data",
            "input_commits_count": len(commits)
        })
        
        data_aggregator = DataAggregatorService()
        # async ë©”ì„œë“œë¥¼ ë™ê¸°ì ìœ¼ë¡œ ì‹¤í–‰
        import asyncio
        try:
            # ì´ë¯¸ ì´ë²¤íŠ¸ ë£¨í”„ê°€ ì‹¤í–‰ ì¤‘ì¸ ê²½ìš° ìƒˆ ë£¨í”„ ìƒì„±
            loop = asyncio.get_event_loop()
            if loop.is_running():
                import concurrent.futures
                with concurrent.futures.ThreadPoolExecutor() as executor:
                    future = executor.submit(asyncio.run, data_aggregator.aggregate_data(aggregation_input))
                    aggregated_data = future.result()
            else:
                aggregated_data = asyncio.run(data_aggregator.aggregate_data(aggregation_input))
        except RuntimeError:
            # ì´ë²¤íŠ¸ ë£¨í”„ê°€ ì—†ëŠ” ê²½ìš°
            aggregated_data = asyncio.run(data_aggregator.aggregate_data(aggregation_input))
        
        log_module_io("DataAggregator", "OUTPUT", output_data=aggregated_data, metadata={
            "developers_processed": len(aggregated_data.developer_stats),
            "repositories_processed": len(aggregated_data.repository_stats),
            "time_analysis_completed": aggregated_data.time_analysis is not None,
            "complexity_metrics_completed": aggregated_data.complexity_metrics is not None
        })
        
        logger.info(
            "âœ… DataAggregator: Processed %d developers, %d repositories",
            len(aggregated_data.developer_stats),
            len(aggregated_data.repository_stats)
        )
        
        # 3ï¸âƒ£ PromptBuilder: LLMìš© í”„ë¡¬í”„íŠ¸ ìƒì„±
        logger.info("ğŸ“ PromptBuilder: Starting prompt generation")
        
        log_module_io("PromptBuilder", "INPUT", input_data=aggregated_data, metadata={
            "operation": "build_daily_summary_prompt",
            "template_type": "daily_developer_summary",
            "language": "korean"
        })
        
        prompt_builder = PromptBuilderService()
        
        # ì „ì²´ ê°œë°œì ëª©ë¡ì—ì„œ ì²« ë²ˆì§¸ ê°œë°œìë¥¼ ëŒ€ìƒìœ¼ë¡œ í•˜ê±°ë‚˜, 
        # ê°œë°œìê°€ ì—†ìœ¼ë©´ "ì „ì²´ íŒ€"ìœ¼ë¡œ ì„¤ì •
        if aggregated_data.developer_stats:
            # ì²« ë²ˆì§¸ ê°œë°œì ì„ íƒ
            first_developer = list(aggregated_data.developer_stats.values())[0]
            target_developer = first_developer.developer
        else:
            target_developer = "ê°œë°œíŒ€"
        
        # AggregationResultë¥¼ ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜
        aggregated_dict = {
            "developer_stats": {
                email: {
                    "developer": stats.developer,
                    "commit_count": stats.commit_count,
                    "lines_added": stats.lines_added,
                    "lines_deleted": stats.lines_deleted,
                    "files_changed": stats.files_changed,
                    "languages_used": stats.languages_used,
                    "avg_complexity": stats.avg_complexity
                }
                for email, stats in aggregated_data.developer_stats.items()
            }
        }
        
        prompt_result = prompt_builder.build_daily_summary_prompt(aggregated_dict, target_developer)
        
        log_module_io("PromptBuilder", "OUTPUT", output_data=prompt_result, metadata={
            "prompt_length": len(prompt_result.prompt),
            "template_used": prompt_result.template_used,
            "variables_count": len(prompt_result.context_data) if hasattr(prompt_result, 'context_data') else 0
        })
        
        logger.info(
            "âœ… PromptBuilder: Generated prompt (%d chars) using template '%s'",
            len(prompt_result.prompt),
            prompt_result.template_used
        )
        
        # 4ï¸âƒ£ LLMService: ì½”ë“œ ë¶„ì„ ë° ìš”ì•½ ìƒì„±
        logger.info("ğŸ¤– LLMService: Starting LLM analysis")
        
        log_module_io("LLMService", "INPUT", input_data=prompt_result, metadata={
            "operation": "generate_summary",
            "prompt_length": len(prompt_result.prompt),
            "provider": "openai"  # ê¸°ë³¸ê°’
        })
        
        llm_service = LLMService()
        
        # LLMInput ê°ì²´ ìƒì„±
        model_config = ModelConfig(
            model="gpt-3.5-turbo",  # ê¸°ë³¸ ëª¨ë¸
            temperature=0.7,
            max_tokens=1000
        )
        
        llm_input = LLMInput(
            prompt=prompt_result.prompt,
            llm_provider=LLMProvider.OPENAI,
            model_config=model_config
        )
        
        # async ë©”ì„œë“œë¥¼ ë™ê¸°ì ìœ¼ë¡œ ì‹¤í–‰
        try:
            # ì´ë¯¸ ì´ë²¤íŠ¸ ë£¨í”„ê°€ ì‹¤í–‰ ì¤‘ì¸ ê²½ìš° ìƒˆ ë£¨í”„ ìƒì„±
            loop = asyncio.get_event_loop()
            if loop.is_running():
                import concurrent.futures
                with concurrent.futures.ThreadPoolExecutor() as executor:
                    future = executor.submit(asyncio.run, llm_service.generate_summary(llm_input))
                    llm_response = future.result()
            else:
                llm_response = asyncio.run(llm_service.generate_summary(llm_input))
        except RuntimeError:
            # ì´ë²¤íŠ¸ ë£¨í”„ê°€ ì—†ëŠ” ê²½ìš°
            llm_response = asyncio.run(llm_service.generate_summary(llm_input))
        
        log_module_io("LLMService", "OUTPUT", output_data=llm_response, metadata={
            "response_length": len(llm_response.summary),
            "confidence_score": llm_response.confidence_score,
            "provider_used": llm_response.metadata.provider if llm_response.metadata else "unknown",
            "model_used": llm_response.metadata.model_used if llm_response.metadata else "unknown"
        })
        
        logger.info(
            "âœ… LLMService: Generated summary (%d chars) using %s/%s",
            len(llm_response.summary),
            llm_response.metadata.provider if llm_response.metadata else "unknown",
            llm_response.metadata.model_used if llm_response.metadata else "unknown"
        )
        
        # 5ï¸âƒ£ NotificationService: Slack ì•Œë¦¼ ì „ì†¡
        logger.info("ğŸ“¢ NotificationService: Starting notification delivery")
        
        # ì•Œë¦¼ ë©”ì‹œì§€ êµ¬ì„±
        notification_message = f"""ğŸŒ… **ì¼ì¼ ê°œë°œ í™œë™ ìš”ì•½** - {yesterday_start.strftime('%Yë…„ %mì›” %dì¼')}

{llm_response.summary}

---
ğŸ“Š **ì „ì²´ í†µê³„**
â€¢ ê°œë°œì: {len(aggregated_data.developer_stats)}ëª…
â€¢ ì €ì¥ì†Œ: {len(aggregated_data.repository_stats)}ê°œ
â€¢ ì²˜ë¦¬ëœ ì»¤ë°‹: {len(commits)}ê°œ

_CodePing.AI ìë™ ìƒì„± ë¦¬í¬íŠ¸_"""
        
        log_module_io("NotificationService", "INPUT", metadata={
            "operation": "send_slack_message",
            "message_length": len(notification_message),
            "channel": "default"
        })
        
        notification_service = NotificationService()
        notification_result = notification_service.send_daily_summary(
            summary_report=notification_message,
            developer=target_developer,
            developer_email="team@codeping.ai",  # ê¸°ë³¸ ì´ë©”ì¼
            slack_channel="#dev-reports"  # ê¸°ë³¸ ì±„ë„
        )
        
        log_module_io("NotificationService", "OUTPUT", output_data=notification_result, metadata={
            "success": notification_result.success,
            "channel_used": notification_result.channel,
            "delivery_time": notification_result.timestamp.isoformat() if notification_result.timestamp else None
        })
        
        if notification_result.success:
            logger.info(
                "âœ… NotificationService: Message sent successfully to %s",
                notification_result.channel
            )
        else:
            logger.error(
                "âŒ NotificationService: Failed to send message: %s",
                notification_result.error_message
            )
        
        # ì‹¤í–‰ ì™„ë£Œ
        total_execution_time = (datetime.now() - start_time).total_seconds()
        
        result = {
            "success": True,
            "date_processed": yesterday_start.strftime('%Y-%m-%d'),
            "developers_count": len(aggregated_data.developer_stats),
            "commits_count": len(commits),
            "notification_sent": notification_result.success,
            "execution_time_seconds": total_execution_time,
            "modules_executed": ["DataRetriever", "DataAggregator", "PromptBuilder", "LLMService", "NotificationService"]
        }
        
        logger.info(
            "ğŸ‰ Daily summary completed successfully in %.2fs: %d developers, %d commits, notification=%s",
            total_execution_time,
            len(aggregated_data.developer_stats),
            len(commits),
            "âœ…" if notification_result.success else "âŒ"
        )
        
        return result
        
    except Exception as exc:
        total_execution_time = (datetime.now() - start_time).total_seconds()
        
        logger.exception("âŒ Daily summary task failed: %s", exc)
        
        return {
            "success": False,
            "error": str(exc),
            "error_type": type(exc).__name__,
            "execution_time_seconds": total_execution_time
        }
